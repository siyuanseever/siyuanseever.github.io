---
---

@string{aps = {American Physical Society,}}

@article{liang2025recurrent,
  title={Block-wise Recurrent Transformer: Enabling Effective Length Extrapolation},
  author={Liang, Siyuan},
  year={2025},
  journal={arXiv preprint},
  pdf={recurrent_transformer.pdf},
  preview={recurrent_transformer.jpg},
  bibtex_show={true},
  selected={true},
  abstract={The Transformer architecture, especially in decoder-only form for autoregressive language modeling, has achieved remarkable success in natural language processing due to its powerful parallel capabilities and attention mechanism. However, the standard Transformer's attention mechanism is stateless, which poses dual challenges of O(N^2) computational complexity and inference memory consumption when processing long texts. Existing solutions such as Transformer-XL introduce segment-level recurrence, but their state transfer is limited to direct copying of the KV cache, lacking deep state evolution, which restricts their theoretical receptive field to N x L. In this paper, we propose a Block-wise Recurrent Transformer architecture (also known as Memory Attention). This architecture transforms the Transformer from a stateless parallel computer into a stateful sequence model by introducing an explicit Recurrent State. Our core innovations lie in: (1) Explicit State Evolution: introducing a non-linear projection (Projection FFN) between blocks, enabling memory to 'think' and 'compress' in the temporal dimension, rather than just being passively stored; and (2) Stateful Segment Training: combining the classic TBPTT idea from RNNs, maintaining state transfer across batches during training, enabling the model to learn true long-distance dependencies. Experimental results show that the model, trained on a length of only 256, can generalize to sequences of 4096 or even longer, with loss continuously decreasing as length increases (Train Short, Test Long), demonstrating effective length extrapolation.},
  code={https://github.com/siyuanseever/llama2Rnn}
}

@inproceedings{zhang2025ledit,
  abbr={NeurIPS},
  title={LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding},
  author={Zhang, Shen and Liang, Siyuan and Tan, Yaning and Chen, Zhaowei and Li, Linze and Wu, Ge and Chen, Yuhao and Li, Shuheng and Zhao, Zhenyu and Chen, Caihua and Liang, Jiajun and Tang, Yao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2025},
  url={https://arxiv.org/abs/2503.04344},
  html={https://arxiv.org/abs/2503.04344},
  code={https://github.com/ShenZhang-Shin/LEDiT},
  website={https://shenzhang2145.github.io/ledit/},
  poster={LEDiT_poster.pdf},
  selected={true},
  abstract={Diffusion transformers (DiTs) struggle to generate images at resolutions higher than their training resolutions. The primary obstacle is that the explicit positional encodings(PE), such as RoPE, need extrapolating to unseen positions which degrades performance when the inference resolution differs from training. In this paper, We propose a Length-Extrapolatable Diffusion Transformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs, thereby avoiding PE extrapolation. The key innovation of LEDiT lies in the use of causal attention. We demonstrate that causal attention can implicitly encode global positional information and show that such information facilitates extrapolation. We further introduce a locality enhancement module, which captures fine-grained local information to complement the global coarse-grained position information encoded by causal attention. Experimental results on both conditional and text-to-image generation tasks demonstrate that LEDiT supports up to 4x resolution scaling (e.g., from 256x256 to 512x512), achieving better image quality compared to the state-of-the-art length extrapolation methods. We believe that LEDiT marks a departure from the standard RoPE-based methods and offers a promising insight into length extrapolation.}
}

@article{9508409,
  abbr={IEEE Sensors},
  author={Zhang, Yu and Jiu, Bo and Wang, Penghui and Liu, Hongwei and Liang, Siyuan},
  journal={IEEE Sensors Journal},
  title={An End-to-End Anti-Jamming Target Detection Method Based on CNN},
  year={2021},
  volume={21},
  number={19},
  pages={21817-21828},
  keywords={Jamming;Object detection;Radar;Feature extraction;Radar detection;Convolutional neural networks;Convolution;Target detection;anti-jamming;end-to-end;CNN},
  doi={10.1109/JSEN.2021.3103042},
  url={https://doi.org/10.1109/JSEN.2021.3103042},
  html={https://doi.org/10.1109/JSEN.2021.3103042},
  pdf={JSEN3103042.pdf},
  selected={true}
}

@inproceedings{liang2022simpledg,
  title={SimpleDG: Simple Domain Generalization Baseline Without Bells and Whistles},
  author={Liang, Siyuan},
  booktitle={ECCV Workshop},
  year={2022},
  code={https://github.com/megvii-research/SimpleDG},
  url={https://arxiv.org/abs/2210.14507},
  html={https://arxiv.org/abs/2210.14507},
  pdf={https://arxiv.org/pdf/2210.14507.pdf}
}

@article{liang2019waveform,
  title={Waveform design for cognitive radar in presence of jammer using Stackelberg game},
  author={Liang, Siyuan},
  journal={The Journal of Engineering},
  year={2019},
  url={https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/joe.2019.0621},
  html={https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/joe.2019.0621},
  doi={10.1049/joe.2019.0621}
}
