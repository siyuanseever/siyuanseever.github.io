- title: General Information
  type: map
  contents:
    - name: Full Name
      value: Siyuan Liang (梁思远)
    - name: Position
      value: Algorithm Research Engineer
    - name: Location
      value: Hangzhou, China
    - name: Email
      value: 354560462@qq.com
    - name: Homepage
      value: <a href="https://siyuanseever.github.io">siyuanseever.github.io</a>

- title: Education
  type: time_table
  contents:
    - title: M.S. in Electronic and Communication Engineering
      institution: Xidian University, Xi'an, China
      year: 2016 - 2019
      description:
        - Research-oriented training at the School of Electronic Engineering.
    - title: B.S. in Electronic Information Science and Technology
      institution: Xidian University, Xi'an, China
      year: 2012 - 2016

- title: Work Experience
  type: time_table
  contents:
    - title: Algorithm Researcher
      institution: Megvii / JIIOV (Megvii Incubation), Beijing, China
      year: 2019 - 2025
      description:
        - Research on long-text and memory architectures, advancing recurrent attention and state-continuous training experiments.
        - Verified extrapolation benefits outside training length (longer extrapolation leads to lower loss).
        - Built LLM retrieval/reranking demo, improving nDCG@10 by 20+ points.
        - Open-sourced llama2Rnn.c, providing minimal C inference implementation and long-term memory persistence.
        - Delivered and optimized fingerprint liveness algorithms across multiple projects/modules; refined ModelZoo search/distillation/release processes.
        - Improved cross-project performance by 1-10 points using randmix/blur/resize augmentation and weight averaging.
        - Developed polarized face liveness demo, reducing 2D false negatives from 7% to 0.1% and increasing face detection rate from 86% to 92%.
        - Landed display driver Demura algorithm, optimizing compression and pipeline from 120s to 20s.
        - Implemented XR hand ray function, reducing jitter by ~40% using temporal reference input.

- title: Research Experience
  type: time_table
  contents:
    - title: Long-Term Memory (Block-Recurrent Attention)
      institution: Research Project
      year: 2022 - Present
      description:
        - Proposed block-recurrent attention utilizing KV cache to pass cross-chunk hidden states for long-term memory.
        - Designed state-continuous training passing hidden states across batches with sequential data continuity.
        - Verified real benefits outside training length (lower prediction loss with longer extrapolation), distinct from previous Transformer extrapolation methods.
        - Plug-and-play replacement as an attention module, verified inference length extrapolation on TinyStories (train 256, eval up to 4096).
        - Systematically evaluated baselines (RoPE/NTK scaling, interpolation, truncation), attributing key bottlenecks to position extrapolation and attention entropy dilution.
        - Open-sourced C language minimal inference implementation and long-term memory persistence (llama2Rnn.c).
    - title: LEDiT (Length-Extrapolatable Diffusion Transformer)
      institution: Research Project
      year: 2025
      description:
        - Removed explicit positional encoding to avoid performance degradation from PE extrapolation.
        - Used causal attention to implicitly encode global positions and introduced local enhancement modules for fine-grained information.
        - Achieved up to 4x resolution extrapolation (256x256 -> 512x512) on conditional and text-to-image tasks, outperforming existing extrapolation methods.

- title: Honors and Awards
  type: time_table
  contents:
    - year: 2022
      items:
        - 2nd Place (Dual Track) & 1st Place (Overall), NICO Challenge 2022 (Domain Generalization)
    - year: 2021
      items:
        - 1st Place, LivDet 2021 Fingerprint Liveness Detection Competition

- title: Selected Publications & Open Source
  type: nested_list
  contents:
    - title: Publications
      items:
        - "LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding, NeurIPS 2025."
        - "SimpleDG: Simple Domain Generalization Baseline without Bells and Whistles, ECCV Workshop 2022."
        - "An End-to-End Anti-jamming Target Detection Method based on CNN, IEEE Sensors Journal 2021."
        - "Waveform design for cognitive radar in presence of jammer using Stackelberg game, The Journal of Engineering 2019."
    - title: Open Source Repositories
      items:
        - <a href="https://github.com/siyuanseever/llama2Rnn.c">llama2Rnn.c</a> - C implementation of Memory Attention with demo and training code
        - <a href="https://github.com/ShenZhang-Shin/LEDiT">LEDiT</a> - PyTorch Implementation, NeurIPS 2025
        - <a href="https://github.com/megvii-research/SimpleDG">SimpleDG</a> - Training and test code for ECCV2022 workshop NICO challenge

- title: Skills
  type: list
  contents:
    - <strong>Research:</strong> Long-context modeling, persistent memory, structural analysis, ablation design
    - <strong>Model:</strong> Transformer, RNN, Attention variants, Length extrapolation
    - <strong>Engineering:</strong> PyTorch, C/C++, Inference optimization, Data & Toolchain
    - <strong>Application:</strong> Face liveness, Fingerprint recognition, Demura pipeline, LLM retrieval
