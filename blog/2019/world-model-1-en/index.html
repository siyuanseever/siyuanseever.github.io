<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> World Models (I): The Union of Memory, Perception, Prediction, Evaluation, and Decision | Siyuan Liang </title> <meta name="author" content="Siyuan Liang"> <meta name="description" content="Distinguishing " simulating the world from we present a unified differentiable framework that couples five core modules and discuss frontiers such as spatial intelligence abstract learning long-term memory.> <meta name="keywords" content="long-context-modeling, recurrent-architectures, deep-learning, algorithm-engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://siyuanseever.github.io/blog/2019/world-model-1-en/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Siyuan</span> Liang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">World Models (I): The Union of Memory, Perception, Prediction, Evaluation, and Decision</h1> <p class="post-meta"> Created on March 31, 2019 </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/agi"> <i class="fa-solid fa-hashtag fa-sm"></i> AGI</a>   <a href="/blog/tag/agent"> <i class="fa-solid fa-hashtag fa-sm"></i> Agent</a>   <a href="/blog/tag/world-model"> <i class="fa-solid fa-hashtag fa-sm"></i> World-Model</a>   <a href="/blog/tag/memory"> <i class="fa-solid fa-hashtag fa-sm"></i> Memory</a>   <a href="/blog/tag/perception"> <i class="fa-solid fa-hashtag fa-sm"></i> Perception</a>   <a href="/blog/tag/prediction"> <i class="fa-solid fa-hashtag fa-sm"></i> Prediction</a>   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>First drafted in April 2019 for my M.S. thesis on intelligent radar; revived in Jan 2026 with new insights from LLMs and spatial intelligence. May this note serve fellow travellers on the road to AGI.</p> </blockquote> <p>Before diving in, let us distinguish two concepts: <strong>simulating the world</strong> and <strong>understanding the world</strong>. Modern video-generative models (e.g. Sora, MovieGen) excel at pixel-level <em>simulation</em>, yet do they <em>understand</em> the underlying physics and causality? Borrowing the metaphor of a “unified field theory” from physics, I define a <strong>World Model</strong> as a <strong>differentiable, end-to-end framework</strong> that tightly couples five functions—<strong>memory, perception, prediction, evaluation, and decision</strong>—into a single, learnable closed loop. The goal is not merely photorealistic frames, but a reasoning, interactive <em>mind</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/world_model-480.webp 480w,/assets/img/world-model/world_model-800.webp 800w,/assets/img/world-model/world_model-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/world_model.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Conceptual diagram of a world model </div> <h2 id="prelude">Prelude</h2> <p>The metaphor is borrowed from physics: a unified field theory that merges the four fundamental forces.</p> <blockquote> <p>A <strong>unified model</strong> here means one that fuses memory, perception, prediction, evaluation, and decision into a <strong>single, differentiable, end-to-end architecture</strong>.</p> </blockquote> <p>Below I detail each module and show how to weave them together “organically” (i.e. differentiably). Later sections instantiate the framework on concrete tasks.</p> <h2 id="five-functional-modules">Five Functional Modules</h2> <p>An agent should implement the following:</p> <ul> <li> <strong>Memory</strong> – temporal, causal memory</li> <li> <strong>Perception</strong> – compressive representation</li> <li> <strong>Prediction</strong> – next-state forecasting</li> <li> <strong>Evaluation</strong> – value estimation</li> <li> <strong>Decision</strong> – policy / action selection</li> </ul> <h3 id="1-memory">1. Memory</h3> <blockquote> <p>Memory is <strong>not</strong> passive storage; it <strong>actively</strong> combines the previous memory state $m_{t-1}$ with the current observation $o_t$ to produce an updated state $s_t$ and memory $m_t$.</p> </blockquote> \[s_t,\; m_t \;=\; D\!\big(o_t,\; m_{t-1}\big)\] <p>Any recurrent architecture that preserves long-term causality qualifies—classic RNNs, LSTMs, and recent “Renaissance” hybrids such as RWKV, RetNet, Mamba, etc. In 2023 I hacked <a href="https://github.com/siyuanseever/llama2RNN.c" rel="external nofollow noopener" target="_blank">llama2RNN.c</a> as a toy demo; a longer write-up is forthcoming.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/memoryAttention-480.webp 480w,/assets/img/world-model/memoryAttention-800.webp 800w,/assets/img/world-model/memoryAttention-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/memoryAttention.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Memory-attention mechanism </div> <h3 id="2-perception">2. Perception</h3> <blockquote> <p>Perception compresses high-dimensional observations into abstract states and approximately reconstructs the original signal.</p> </blockquote> \[\hat{o}\;=\;D^{-1}\!\big(D(o)\big)\] <p>The code $D(o)$ must be dramatically smaller than the raw observation $o$. Vanilla auto-encoders or MAE already satisfy this template.</p> <h3 id="3-prediction">3. Prediction</h3> <blockquote> <p>From the abstract state (and any prior) the agent forecasts the <strong>next abstract state</strong>, not the next pixel frame.</p> </blockquote> \[s'_{t+1}\;=\;P(s_t)\] <p>Large language models follow the same principle, except they predict raw tokens rather than states.</p> <h3 id="4-evaluation">4. Evaluation</h3> <blockquote> <p>The agent assigns a scalar <strong>value</strong> to each state, reflecting expected cumulative reward.</p> </blockquote> \[v_t \;=\; E(s_t) \;=\; \mathbb{E}\!\left[r \;+\; \gamma\, E\!\big(s_{t+1}\big)\right]\] <p>This is the value network familiar in RL.</p> <h3 id="5-decision">5. Decision</h3> <blockquote> <p>The agent <strong>acts</strong> to change both the external world and its own internal state.</p> </blockquote> \[\pi(s) \;=\; \arg\max_{a}\, Q(s, a)\] <p>Actions include not only motor commands but also <strong>self-modifications</strong>—e.g. architecture search (NASNet-style), learning-rate updates, or any differentiable controller that rewrites its own parameters.</p> <h2 id="instantiations">Instantiations</h2> <h3 id="a-vision-based-multi-task-manipulation-from-demonstration">A. Vision-based Multi-task Manipulation from Demonstration</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/demonstration-480.webp 480w,/assets/img/world-model/demonstration-800.webp 800w,/assets/img/world-model/demonstration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/demonstration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> End-to-end imitation learning for cheap robot arms </div> <p>The system couples a multi-modal auto-regressive control network with a VAE-GAN reconstructor; the encoder (perception) feeds state features to the controller, yielding a minimal but complete perception–action loop.</p> <h3 id="b-next-state-prediction-instead-of-next-token-prediction">B. Next-State Prediction instead of Next-Token Prediction</h3> <p>If LLMs push <em>next-token</em> prediction to the extreme, <strong>next-state</strong> prediction couples forecasting with perception for data-efficient learning on high-bandwidth modalities such as video.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/next_word_prediction.drawio-480.webp 480w,/assets/img/world-model/next_word_prediction.drawio-800.webp 800w,/assets/img/world-model/next_word_prediction.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/next_word_prediction.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Next-state predictive framework </div> <p>Key references:</p> <ul> <li>Joint Embedding Predictive Architecture (JEPA)</li> <li>Emu3.5</li> </ul> <p>LeCun’s roadmap to autonomous machine intelligence resonates strongly with this line of thought—sadly I still lack the engineering muscle to ship a full-scale demo.</p> <h3 id="c-v-jepa-2-ac-self-supervised-video-understanding--planning">C. V-JEPA 2-AC: Self-supervised Video Understanding &amp; Planning</h3> <p>V-JEPA 2-AC adds <strong>action conditioning</strong> to perception and prediction. Although it does not emit <em>actions</em> directly (evaluation + RL are still needed), it learns to imitate state-action transitions observed in the training videos.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/V-JEPA2-AC-480.webp 480w,/assets/img/world-model/V-JEPA2-AC-800.webp 800w,/assets/img/world-model/V-JEPA2-AC-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/V-JEPA2-AC.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> V-JEPA 2-AC overview </div> <h2 id="frontiers-spatial-intelligence">Frontiers: Spatial Intelligence</h2> <p>Prof. Fei-Fei Li’s team (World Labs) recently popularised <strong>Spatial Intelligence</strong>—a perfect sandbox for world models.</p> <h3 id="1-vision-before-language">1. Vision before Language?</h3> <blockquote> <p>Perception and action became the core loop driving the evolution of intelligence.</p> </blockquote> <p>Even pre-vertebrate animals without language rely on vision to grasp physics (gravity, occlusion) and act. The next leap toward AGI must therefore endow AI with <strong>spatial cognition</strong>, not merely linguistic competence.</p> <h3 id="2-definition">2. Definition</h3> <blockquote> <p>Building frontier models that can <strong>perceive, generate, reason, and interact</strong> with the 3D world.</p> </blockquote> <p>This aligns one-to-one with our five-module taxonomy:</p> <ul> <li> <strong>Perceive</strong> – 3D structure understanding</li> <li> <strong>Generate</strong> – imagine future states</li> <li> <strong>Reason</strong> – causal inference (evaluation + memory)</li> <li> <strong>Interact</strong> – decision-making in physical spaces</li> </ul> <h3 id="3-marble-from-generating-videos-to-generating-worlds">3. Marble: From “Generating Videos” to “Generating Worlds”</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/Marble-480.webp 480w,/assets/img/world-model/Marble-800.webp 800w,/assets/img/world-model/Marble-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/Marble.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Marble: persistent, editable 3D worlds </div> <p>Marble highlights two deficits of video-centric models:</p> <ul> <li> <strong>Spatial inconsistency</strong> – objects drift or vanish; perspective violates physics.</li> <li> <strong>Ephemerality</strong> – pixels disappear; no persistent 3D substrate.</li> </ul> <p>Spatial intelligence demands an <strong>explicit 3D latent state</strong> that respects physics and remains editable. The AI graduates from <em>painter</em> to <em>demiurge</em>.</p> <p>Long-form temporal consistency can also be injected via <strong>long-context memory</strong>, from early ConvLSTM to modern state-space models and my own block-wise recurrent transformer experiments.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/Long-Context_State-Space_Video_World_Models-480.webp 480w,/assets/img/world-model/Long-Context_State-Space_Video_World_Models-800.webp 800w,/assets/img/world-model/Long-Context_State-Space_Video_World_Models-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/Long-Context_State-Space_Video_World_Models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Long-context state-space video world models </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/Long-Context_State-Space_Model_architecture-480.webp 480w,/assets/img/world-model/Long-Context_State-Space_Model_architecture-800.webp 800w,/assets/img/world-model/Long-Context_State-Space_Model_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/Long-Context_State-Space_Model_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> State-space model architecture for long contexts </div> <h2 id="learning-like-humans">Learning like Humans</h2> <p>World models diverge from mainstream deep learning in <strong>data efficiency</strong> and <strong>adaptation</strong>.</p> <ol> <li> <strong>Abstract Learning</strong> – physicians read MRI scans by <em>concepts</em>, not pixels; future AI must exploit spatial commonsense.</li> <li> <strong>Continual Learning</strong> – we should target an <strong>evolving intelligence</strong> that adapts lifelong, rather than a frozen AGI that ships once.</li> <li> <strong>Temporal Awareness</strong> – time is the only unquestionable physical quantity. Any serious model (CNN or Transformer) will eventually re-acquire an <strong>RNN backbone</strong>; without it, entropy and causality remain invisible, precluding true <em>silicon life</em>.</li> </ol> <p>Recurrent inductive biases endow models with <strong>long-term, causal memory</strong>, solving length extrapolation <em>and</em> letting AI accumulate experience across training steps instead of being <em>reformatted</em> after every restart.</p> <h2 id="case-study-intelligent-electromagnetic-game">Case Study: Intelligent Electromagnetic Game</h2> <p>To show that the framework is <em>not</em> limited to video games, I apply it to <strong>radar–jammer adversarial signalling</strong>—a decidedly <em>hardcore</em> domain.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/world-model/intelligent_electromagnetic_game-480.webp 480w,/assets/img/world-model/intelligent_electromagnetic_game-800.webp 800w,/assets/img/world-model/intelligent_electromagnetic_game-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/world-model/intelligent_electromagnetic_game.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Intelligent electromagnetic game: radar vs. jammer </div> <p>My M.S. thesis built a <strong>deep-RL radar agent</strong> implementing the full loop:</p> <ol> <li> <strong>Perception + Memory</strong> – Conv-LSTM ingests pulse echoes, retaining long-term memory of earlier pulses.</li> <li> <strong>Decision</strong> – a policy network $\pi(o_{t-1})$ <em>generates</em> the next transmit waveform instead of using a fixed template.</li> <li> <strong>Evaluation</strong> – a value network $V(o_t)$ predicts the long-term detection return of the chosen waveform under future jamming.</li> <li> <strong>World</strong> – radar and jammer co-train in a <strong>fully differentiable</strong> adversarial channel.</li> </ol> <p>The cycle <strong>transmit (decision) → jamming (world feedback) → echo detection (perception / evaluation)</strong> forms an end-to-end closed loop.</p> <h2 id="epilogue">Epilogue</h2> <p>History offers a constellation of ideas—RL, meta-learning, self-supervised prediction, compressive sensing, RNNs, ResNets, Transformers, NAS, and more. Each has its merits. The AGI of tomorrow will weave them together without disdain, greeting even today’s over-industrialised LLMs with the words:</p> <blockquote> <p>“You have arrived precisely on time.”</p> </blockquote> <hr> <p>Series Navigation</p> <ul> <li>Next: <a href="/blog/2019/intelligent-radar/">World Models (II): Intelligent Electromagnetic Game</a> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/intelligent-radar/">世界模型（二）：智能电磁博弈</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/intelligent-radar-en/">World Models (II): Intelligent Electromagnetic Game</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/world-model-1/">世界模型（一）：记忆、感知、预测、评估、决策的联合</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Siyuan Liang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>