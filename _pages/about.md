---
layout: about
title: about
permalink: /
subtitle: Long-context modeling & recurrent architectures

profile:
  align: right
  image: profile.png
  image_circular: false # crops the image to make it circular

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: false # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

Siyuan Liang (梁思远) focuses on long-context modeling and recurrent architectures for sequence models, and proposed the Block-wise Recurrent Transformer (Memory Attention) for strong length extrapolation under train-short, test-long settings.

Previously, he worked as an algorithm researcher at Megvii in Beijing, delivering production algorithms for fingerprint and face liveness, display demura, and XR hand tracking.

He received his M.S. in Electronic and Communication Engineering from Xidian University, where his research centered on deep-learning-based radar anti-jamming detection and intelligent electromagnetic games.

---

<p><i class="fa-brands fa-github gh-icon"></i> <a href="https://github.com/siyuanseever/llama2RNN.c">llama2RNN.c</a></p>
<p><i class="fa-brands fa-github gh-icon"></i> <a href="https://github.com/shenzhang2145/ledit">LEDiT</a></p>
<p><i class="fa-brands fa-github gh-icon"></i> <a href="https://github.com/megvii-research/SimpleDG">SimpleDG</a></p>
